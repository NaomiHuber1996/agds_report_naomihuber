---
title: "Report Exercise Chapter 9"
author: "Naomi Huber"
output: html_document
toc: true
---

## Load data
Reading the data, selecting the suitable variables, interpretation of missing value codes and selection of only good-quality data.
```{r}
library(lubridate)
library(dplyr)
library(readr)

daily_fluxes <- read_csv("https://raw.githubusercontent.com/geco-bern/agds/main/data/FLX_CH-Dav_FLUXNET2015_FULLSET_DD_1997-2014_1-3.csv") |>  
  
  # select only the variables we are interested in
  dplyr::select(TIMESTAMP,
                GPP_NT_VUT_REF,    # the target
                ends_with("_QC"),  # quality control info
                ends_with("_F"),   # includes all all meteorological covariates
                -contains("JSB")   # weird useless variable
  ) |>
  
  # convert to a nice date object
  dplyr::mutate(TIMESTAMP = ymd(TIMESTAMP)) |>
  
  # set all -9999 to NA
  mutate(across(where(is.numeric), ~na_if(., -9999))) |>
  
  # and retain only data based on >=80% good-quality measurements
  # overwrite bad data with NA (not dropping rows)
  dplyr::mutate(GPP_NT_VUT_REF = ifelse(NEE_VUT_REF_QC < 0.8, NA, GPP_NT_VUT_REF),
                TA_F           = ifelse(TA_F_QC        < 0.8, NA, TA_F),
                SW_IN_F        = ifelse(SW_IN_F_QC     < 0.8, NA, SW_IN_F),
                LW_IN_F        = ifelse(LW_IN_F_QC     < 0.8, NA, LW_IN_F),
                VPD_F          = ifelse(VPD_F_QC       < 0.8, NA, VPD_F),
                PA_F           = ifelse(PA_F_QC        < 0.8, NA, PA_F),
                P_F            = ifelse(P_F_QC         < 0.8, NA, P_F),
                WS_F           = ifelse(WS_F_QC        < 0.8, NA, WS_F)) |> 
  
  # drop QC variables
  dplyr::select(-ends_with("_QC"))
```
## Data Cleaning
```{r}
library(ggplot2)
# Data cleaning: looks ok, no obviously bad data
# no long tail, therefore no further target engineering
daily_fluxes |> 
  ggplot(aes(x = GPP_NT_VUT_REF, y = ..count..)) + 
  geom_histogram()
```


## Split data 
For the calculations, the data has to be splitted into testing data and into trainings data.
```{r}
set.seed(123)
split <- rsample::initial_split(daily_fluxes, prop = 0.7, strata = "VPD_F")
daily_fluxes_train <- rsample::training(split)
daily_fluxes_test <- rsample::testing(split)
```

## Specify model
The steps above are followed by model and pre-processing formulation incl. drop of LW_IN_F:
```{r}
library(tidyr)

pp <- recipes::recipe(GPP_NT_VUT_REF ~ SW_IN_F + VPD_F + TA_F, 
                      data = daily_fluxes_train |> drop_na()) |> 
  #recipes::step_BoxCox(all_predictors()) |> 
  recipes::step_center(all_numeric(), -all_outcomes()) |>
  recipes::step_scale(all_numeric(), -all_outcomes())
```

## Fit regression model
As a next step follows the formulation of the linear regression model:
```{r}
library(recipes)
mod_lm <- caret::train(
  pp, 
  data = daily_fluxes_train |> drop_na(), 
  method = "lm",
  trControl = caret::trainControl(method = "none"),
  metric = "RMSE"
)
```

## Fit the KNN model
And then the formulation of the KNN-model:
```{r}
mod_knn <- caret::train(
  pp, 
  data = daily_fluxes_train |> drop_na(), 
  method = "knn",
  trControl = caret::trainControl(method = "none"),
  tuneGrid = data.frame(k = 8),
  metric = "RMSE"
)
```

```{r}
source("https://raw.githubusercontent.com/NaomiHuber1996/agds_report_naomihuber/main/vignettes/re_ml_01_functions/function_evalmodel.R")
```

## Visualizations
```{r}
# linear regression model
eval_model(mod = mod_lm, df_train = daily_fluxes_train, df_test = daily_fluxes_test)

# KNN
eval_model(mod = mod_knn, df_train = daily_fluxes_train, df_test = daily_fluxes_test)
```
Unfortunately, I have always the following error message which shows up if I want to knit this code. In the .Rmd file it works but I cannot transform it into a .html-file. 

## Interpretation of the observed differences in the context of the bias-variance trade-off
#### Why is the difference between the evaluation on the training and the test set larger for the KNN model than for the linear regression model?
The difference between the evaluation of the training and the test set ist the "overfitting". This means, that a model is complex and matches too much with the training data. The generalization of the new data is thus poor. The KNN model is non-parametric and can overfit to the training data when the number of the neighbours is too small and therefore the new data can only be badly performed. The linear regression model is a parametric model and is therefore less likely to overfit. The larger the difference between the evaluation of the two set may be because of the higher tendency to overfit. 

#### Why does the evaluation on the test set indicate a better model performance of the KNN model than the linear regression model?
The KNN model can better take up nonlinear relationships between the feature and the traget variable. The linear regression model makes a linear relationship between the feature and the traget variable. When the real relatioinship between the feature and the target variable is nonlinear, the KNN is better and makes more accurate predictions in the test set.

#### How would you position the KNN and the linear regression model along the spectrum of the bias-variance trade-off?
The KNN has a high variance and a low bias. The reason is because the KNN is sensitive to noise and can overfit the training data. When k decreases, the model becomes more complex and has a high variance.
The linear regression has a low variance and a high bias. This is because it is a linear model and therefore also the relationship between the variables is linear. 

## Visualization of the temporal variations of observed and modelled GPP for both models
```{r}
library(ggplot2)
library(dplyr)

plotfunction <- function(mod, df_test) {

df_test_new <- df_test |> 
  drop_na()

df_test_new$fitted <- predict(mod, newdata = df_test_new)

tabelle <- tibble("Timestamp" = df_test$TIMESTAMP,
                  "Difference" = df_test$GPP_NT_VUT_REF - df_test_new$fitted)
return(tabelle)
}

visualization <- plotfunction(mod_knn, df_test = daily_fluxes_test)
```
```{r}
ggplot(data = visualization) +
         geom_line(aes(x = Timestamp, y = Difference)) 
         
```


## The role of k
#### Based on the understanding of KNN, state a hypothesis for how R^2 and the MAE evaluated on the test and on the training set would change for k approaching 1 and for k approaching N. 
If k approaches to 1, the model becomes more flexible and the variance increases. But this may also lead to overfitting to the data and the generalization will be poor. Therefore, the R^2 will be lower and the MAE will be higher on the test set compared to the training set. If k approaches to N, the model is less flexible and will maybe underfit. This means that the R^2 will be high and the MAE will be low in the training and the test set. To get the best result, k must be somewhere in the middle where also the bias-variance trade-off will be balanced.

#### Put the hypothesis to the test. Write a code that splits the data into a training and a test set and repeats model fitting and evaluation for different values for k: Visualise results, showing model generalisability as a function of model complexity. Describe how a "region" of overfitting and underfitting can be determined in the visualisation. Write code into a function that takes k as an input and returns the RMSE determined on the test set.
```{r}
library(recipes)
library(caret)

k_values <- seq(1, 50, by = 1)

# definition of the vectors for saving RMSE-values 
train_rmse_values <- rep(NA, length(k_values))
test_rmse_values <- rep(NA, length(k_values))

# iterating over k
for (i in seq_along(k_values)) {
  k <- k_values[i]
  
  # adjust the model to the training data and saving the RMSE-value
  mod_knn_train <- caret::train(
    pp, 
    data = daily_fluxes_train |> drop_na(), 
    method = "knn",
    trControl = caret::trainControl(method = "cv"),
    tuneGrid = data.frame(k = k),
    metric = "RMSE"
  )
  train_rmse_values[i] <- mod_knn_train$results$RMSE
  
  # adjust the model to the test data and saving the RMSE-value
  mod_knn_test <- caret::train(
    pp, 
    data = daily_fluxes_test |> drop_na(), 
    method = "knn",
    trControl = caret::trainControl(method = "cv"),
    tuneGrid = data.frame(k = k),
    metric = "RMSE"
  )
  test_rmse_values[i] <- mod_knn_test$results$RMSE
}

# k with the best RMSE-value
best_k <- k_values[which.min(test_rmse_values)]
cat("The best k-value is: ", best_k)
```

The region of overfitting and underfitting can be visualised by adding of a line into the plot. The region which is underfitting can be determined when the model has a high training and high testing error. The region which is overfitting can be found by a extremely low training but a high testing error. Regarding our example, the region which is underfitting is for example all of the k under 34 (best_k-value) and overfitting is the region with k higher than 34 (where the model fits too closely).



#### Is there an "optimal" k in terms of model generalisability?
Yes. As seen on the graph the optimal k is 34. To get the best k out of the graph, we do it as follows:
```{r}
library(ggplot2)

# getting the dataframe for plotting the data
df <- data.frame(k = k_values, 
                 train_rmse = train_rmse_values, 
                 test_rmse = test_rmse_values)

# generate the plot
ggplot(data = df, aes(x = k)) + 
  geom_line(aes(y = train_rmse, color = "Train")) + 
  geom_line(aes(y = test_rmse, color = "Test")) + 
  scale_color_manual(values = c("Train" = "blue", "Test" = "red")) + 
  geom_vline(xintercept = best_k, linetype = "dashed", col = "green3")
  labs(title = "RMSE for different k values", 
       x = "k", y = "RMSE")
```
Regarding the graph now, we can assume that the optimal k in our data is between 30 and 40, because it is the lowest one. To be precise, k is more or less inbetween and is approximately 34 (green line). 
